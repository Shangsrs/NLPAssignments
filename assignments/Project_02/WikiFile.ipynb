{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import jieba\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from langconv import *\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import logging\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_file =  '/python/datasource/wiki_data/data/wikireduce.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_process(wiki_file_name,cut_file):\n",
    "    space = ' '\n",
    "    i = 0\n",
    "    l = []        \n",
    "    f = open(cut_file,'a',encoding = 'utf-8')\n",
    "    wikiFile = open(wiki_file_name,'r',encoding='utf-8')\n",
    "    for text in wikiFile:\n",
    "        text = text.strip()\n",
    "        text = str(text)\n",
    "        if not text: continue\n",
    "        if re.match(r'<.*>',text): continue\n",
    "        temp_sentence = Converter('zh-hans').convert(text)\n",
    "        temp_sentence = \"\".join(re.findall(r'\\w+',temp_sentence)        )\n",
    "        seg_list = list(jieba.cut(temp_sentence))\n",
    "        for temp_term in seg_list:\n",
    "            l.append(temp_term)        \n",
    "        f.write(space.join(l) + '\\n')\n",
    "        l = []\n",
    "        i = i + 1\n",
    "        if (i%20000 == 0):\n",
    "            print('Saved ' + str(i) + 'lines')            \n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    wiki_news = open(cut_file, 'r',encoding='utf-8')\n",
    "    model = Word2Vec(LineSentence(wiki_news),sg=0, size=300, window=5,min_count=5, workers=4)\n",
    "    model.save('D:\\python\\datasource\\wiki_data\\wiki_data.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_dir = '/python/datasource/wiki_data/data/extract/AA'\n",
    "cut_dir = '/python/datasource/wiki_data/data/'\n",
    "pathDir = os.listdir(wiki_dir)\n",
    "for f in pathDir:\n",
    "    print(f)\n",
    "    child = os.path.join('%s/%s'%(wiki_dir, f))\n",
    "#     cut_file = os.path.join('%s/%s.txt'%(cut_dir, f))\n",
    "    wiki_process(child, cut_file)\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(\"/python/datasource/wiki_data/wiki_data.word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import jieba\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connect_graph_by_text_rank(tokenized_text, window=3):\n",
    "    keywords_graph = nx.Graph()\n",
    "    tokeners = tokenized_text.split()\n",
    "    for ii, t in enumerate(tokeners):\n",
    "        word_tuples = [(tokeners[connect], t) \n",
    "                       for connect in range(ii-window, ii+window+1) \n",
    "                       if connect >= 0 and connect < len(tokeners)]\n",
    "        keywords_graph.add_edges_from(word_tuples)\n",
    "\n",
    "    return keywords_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextRank(cut_sents):\n",
    "    #connect graph\n",
    "    words_graph = get_connect_graph_by_text_rank(\" \".join(cut_sents[0]))\n",
    "\n",
    "    #draw networkx\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "    plt.figure(3, figsize=(12, 12))\n",
    "    nx.draw_networkx(words_graph, font_size=12)\n",
    "\n",
    "    #get dictory\n",
    "    g_dic = {}\n",
    "    node_set = set()\n",
    "    for i, j in words_graph.edges():\n",
    "        node_set.add(i)\n",
    "        node_set.add(j)\n",
    "        if i == j: continue\n",
    "        if i not in g_dic : g_dic[i] = set()\n",
    "        g_dic[i].add(j)\n",
    "\n",
    "    #matrics\n",
    "    node_mat = np.zeros((len(node_set), len(node_set)))\n",
    "    for i, w1 in enumerate(g_dic):\n",
    "        p = 1/len(g_dic[w1])\n",
    "        for j, w2 in enumerate(g_dic[w1]):\n",
    "            node_mat[j][i] = p\n",
    "    node_mat\n",
    "\n",
    "    #text rank\n",
    "    d = 0.85\n",
    "    pr = np.ones((len(node_set), 1)) \n",
    "    for i in range(10):\n",
    "        print(pr.T)\n",
    "        pr = 1/len(node_set) * (1-d) + d * np.dot(node_mat,pr)\n",
    "    print('\\n',pr.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = '/python/datasource/sqlResult_1558435.csv'\n",
    "\n",
    "news = pd.read_csv(fname, encoding = 'gb18030')\n",
    "len(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
