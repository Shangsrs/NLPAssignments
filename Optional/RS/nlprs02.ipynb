{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mdata\u001b[m\u001b[m          nlprs02.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "#一句话预处理行为日志\n",
    "\n",
    "!ls\n",
    "!cat ./data/ratings_Beauty.csv | grep -v UserId | sort -t',' -nk4 | awk 'BEGIN{FS=\",\";OFS=\"\\t\"}$3>2{d[$1]=d[$1]$2\";\"}END{for(i in d) print i\"\\t\"substr(d[i],0,length(d[i])-1)}' | awk '{split($2,a,\";\");if(length(a)>3)print $2}' | awk 'gsub(\";\",\"\\t\",$0)' > ./data/corpus.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用gensim计算商品的特征向量\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import *\n",
    "import multiprocessing\n",
    "\n",
    "#向量维度\n",
    "v_size = 8\n",
    "#窗口大小\n",
    "v_window = 3\n",
    "#最小频率\n",
    "v_min_count = 2\n",
    "v_workers = 2\n",
    "\n",
    "corpusFilePath = './data/corpus.dat'\n",
    "\n",
    "corpusFile = open(corpusFilePath, u'r')\n",
    "model = Word2Vec(\n",
    "    LineSentence(corpusFile), \n",
    "    size=v_size, \n",
    "    window=v_window, \n",
    "    min_count=v_min_count, workers=v_workers)\n",
    "\n",
    "modelPath = './data/model.dat'\n",
    "model.wv.save_word2vec_format(modelPath, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用annoy建立索引\n",
    "\n",
    "import csv\n",
    "from annoy import AnnoyIndex\n",
    " \n",
    "#向量的维度\n",
    "f = 8\n",
    "#新建索引的实例\n",
    "t = AnnoyIndex(f)\n",
    "\n",
    "#商品序号\n",
    "NR = 0\n",
    "#商品序号 -> 商品ID\n",
    "nr2name = {}\n",
    "with open('./data/model.dat') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=' ')\n",
    "    for line in csv_reader:\n",
    "\n",
    "        NR += 1\n",
    "\n",
    "        if len(line) != 9: continue\n",
    "\n",
    "        try:\n",
    "            trigger = line[0]\n",
    "            line = line[1:]\n",
    "            v = [float(dim) for dim in line]\n",
    "            t.add_item(NR, v)\n",
    "            nr2name.setdefault(NR, trigger)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error\")  \n",
    "\n",
    "#用20棵树构造索引\n",
    "t.build(20)\n",
    "t.save('./data/ann_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取模型文件\n",
    "u = AnnoyIndex(f)\n",
    "u.load('./data/ann_index')\n",
    "\n",
    "#展示最近邻\n",
    "for NR,name in nr2name.items():\n",
    "    \n",
    "    if NR > 5: break\n",
    "        \n",
    "    print(name + \"最相近的item: \")\n",
    "    \n",
    "    r = u.get_nns_by_item(NR, 5, include_distances=False)\n",
    "    for item in r:\n",
    "        print(\"\\t\" + nr2name.get(item))\n",
    "        \n",
    "#作业一：\n",
    "#从这里下载Metadata：http://jmcauley.ucsd.edu/data/amazon/\n",
    "#来验证通过向量找到的最近邻是否符合预期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hnsw的参考文献：\n",
    "#Malkov Y A, Yashunin D A. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs[J]. IEEE transactions on pattern analysis and machine intelligence, 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下是DSSM模型相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Activation, Input, Embedding, Flatten, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dense, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import dot\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_input (InputLayer)         [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item_input (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_19 (Embedding)        (None, 5, 8)         800000      user_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 1, 8)         800000      item_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embedding_21 (Embedding)        (None, 1, 8)         800000      input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_22 (Embedding)        (None, 1, 8)         800000      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_23 (Embedding)        (None, 1, 8)         800000      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, 1, 8)         800000      input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 8)            0           embedding_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_15 (Flatten)            (None, 8)            0           embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_16 (Flatten)            (None, 8)            0           embedding_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_17 (Flatten)            (None, 8)            0           embedding_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_18 (Flatten)            (None, 8)            0           embedding_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_19 (Flatten)            (None, 8)            0           embedding_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "ufc (Dense)                     (None, 8)            72          global_average_pooling1d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "ifc (Dense)                     (None, 8)            72          flatten_15[0][0]                 \n",
      "                                                                 flatten_16[0][0]                 \n",
      "                                                                 flatten_17[0][0]                 \n",
      "                                                                 flatten_18[0][0]                 \n",
      "                                                                 flatten_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dot_15 (Dot)                    (None, 1)            0           ufc[0][0]                        \n",
      "                                                                 ifc[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_16 (Dot)                    (None, 1)            0           ufc[0][0]                        \n",
      "                                                                 ifc[1][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_17 (Dot)                    (None, 1)            0           ufc[0][0]                        \n",
      "                                                                 ifc[2][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_18 (Dot)                    (None, 1)            0           ufc[0][0]                        \n",
      "                                                                 ifc[3][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dot_19 (Dot)                    (None, 1)            0           ufc[0][0]                        \n",
      "                                                                 ifc[4][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 5)            0           dot_15[0][0]                     \n",
      "                                                                 dot_16[0][0]                     \n",
      "                                                                 dot_17[0][0]                     \n",
      "                                                                 dot_18[0][0]                     \n",
      "                                                                 dot_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 5)            0           concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 4,800,144\n",
      "Trainable params: 144\n",
      "Non-trainable params: 4,800,000\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#DSSM\n",
    "\n",
    "#模型结构\n",
    "def dssm(item_voc_size, item_embeddings, seq_len=5, dim=8, J=4):\n",
    "    \n",
    "    #模型输入\n",
    "    user_input = Input(shape=(seq_len,), name='user_input')\n",
    "    pos_item_input = Input(shape=(1,), name='item_input')\n",
    "    neg_item_inputs = [Input(shape=(1,)) for _ in range(J)]\n",
    "\n",
    "    #user塔\n",
    "    user_embedding = Embedding(item_voc_size, dim, weights=[item_embeddings], input_length=seq_len, trainable=False)(user_input)\n",
    "    user_avg_pooling = GlobalAveragePooling1D()(user_embedding)\n",
    "    user_fc = Dense(8, activation='relu', name='ufc')(user_avg_pooling)\n",
    "\n",
    "    #item塔\n",
    "    pos_item_embedding = Embedding(item_voc_size, dim, weights=[item_embeddings], trainable=False)(pos_item_input)\n",
    "    neg_item_embeddings = [Embedding(item_voc_size, dim, weights=[item_embeddings], trainable=False)(neg_item_input) for neg_item_input in neg_item_inputs]\n",
    "\n",
    "    pos_item_flatten = Flatten()(pos_item_embedding)\n",
    "    neg_item_flattens = [Flatten()(neg_item_embedding) for neg_item_embedding in neg_item_embeddings]\n",
    "\n",
    "    item_fc = Dense(8, activation='relu', name='ifc')\n",
    "\n",
    "    pos_item_fc = item_fc(pos_item_flatten)\n",
    "    neg_item_fcs = [item_fc(neg_item_flatten) for neg_item_flatten in neg_item_flattens]\n",
    "\n",
    "    user_item_pos_output = dot([user_fc, pos_item_fc], axes=1, normalize=True)\n",
    "    user_item_neg_outputs = [dot([user_fc, neg_item_fc], axes=1, normalize=True) for neg_item_fc in neg_item_fcs]\n",
    "\n",
    "    outputs = concatenate([user_item_pos_output] + user_item_neg_outputs)\n",
    "\n",
    "    prob = Activation(\"softmax\")(outputs)\n",
    "\n",
    "    model = Model(inputs=[user_input, pos_item_input] + neg_item_inputs, outputs=prob)\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#作业二：一个基本的dssm模型结构已经搭建完成（如上面代码）。\n",
    "#现在你可以尝试根据模型所需的输入格式，从原始行为日志生成input data（答案下节课讲）\n",
    "item_voc_size = 100000\n",
    "emb_dim = 8\n",
    "item_embeddings = np.zeros((item_voc_size, emb_dim))\n",
    "model = dssm(item_voc_size, item_embeddings, seq_len=5)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
